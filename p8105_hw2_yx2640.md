p8105\_hw2\_yx2640
================
Elaine Xu
9/30/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages ----------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts -------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

# Problem 1

First, define a path to the dataset.

``` r
path_to_data = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx"
```

Read the Mr. Trashwheel dataset.

``` r
trashwheel_df = 
    read_xlsx(
        path = path_to_data,
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls)
    )
```

Read precipitation data for 2018 and 2017.

``` r
precip_2018 = 
    read_excel(
        path = path_to_data,
        sheet = "2018 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2018) %>% 
    relocate(year)

precip_2017 = 
    read_excel(
        path = path_to_data,
        sheet = "2017 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2017) %>% 
    relocate(year)
```

Now combine annual precipitation dataframes.

``` r
month_df = 
    tibble(
        month = 1:12,
        month_name = month.name
    )

precip_df = 
    bind_rows(precip_2018, precip_2017)

precip_df =
    left_join(precip_df, month_df, by = "month")
```

##### The dataset contains information on year, month, and trash collected, include some specific kinds of trash. There are a total of 344 rows in the final dataset after omit non-data entries. Additional data sheets include month precipitation data. In this dataset:

##### \* The median number of sports balls found in a dumpster in 2017 was 8

##### \* The total precipitation in 2018 was 70.33 inches.

# Problem 2

Read the NYC transit dataset.

``` r
NYC_df = 
    read_csv(
        "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv"
        )%>%
  janitor::clean_names()%>%
  select(c(2:18, 20, 23))%>%
  mutate(entry = as.logical(recode(entry, "YES" = 'TRUE', "NO" = 'FALSE')))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
names(NYC_df)
```

    ##  [1] "line"              "station_name"      "station_latitude" 
    ##  [4] "station_longitude" "route1"            "route2"           
    ##  [7] "route3"            "route4"            "route5"           
    ## [10] "route6"            "route7"            "route8"           
    ## [13] "route9"            "route10"           "route11"          
    ## [16] "entrance_type"     "entry"             "vending"          
    ## [19] "ada"

``` r
#skimr::skim(NYC_df)
```

##### \* This dataset focuses on NYC transit data that contains the entrances and exits data for different subway stations. The dataset was originally include 32 columns of data. We pulled out columns includes the line and station name, station latitude and longitude, routes served, entry and entrance type, vending and ADA to create a new dataset for analysis. The resulting dataset has 19 columns and 1868 rows.

##### \* After import the data, using clean\_names() function to clean the variable names. By pulling out 19 columns from the original data, we recreate a data set that is smaller, which can help us focusing on the data we need. We also change the varible typr in the “entry” column from character variable (YES/NO) to logical variable (TRUE/FALSE).

##### \* The data was not very tidy by looking at the columns through “route1” to “route11”. There are many empty spacies that was expressing the same thing.

Find distinct stations, stations with ADA compliant, proportion of
station entrances/ exits without vending allow entrance.

``` r
#Distinct Stations
dis_line_stat = distinct(NYC_df, line, station_name, .keep_all = TRUE)

#Stations with ADA compliant
Station_ADA = sum(pull(dis_line_stat, ada), na.rm = TRUE)

#Proportion
proportion = nrow(filter(NYC_df, vending == "NO", entry == "TRUE"))/nrow(filter(NYC_df, vending == "NO"))
```

##### There are 465 dustunct stations. 84 of the stations are ADA compliant. The proportion of station entrances/ exits without vending allow entrance is 37.704918%.

``` r
#Reformat data so that route number and route name are distinct variables
Route_line = NYC_df %>%
  mutate(route8 = as.character(route8),
         route9 = as.character(route9),
         route10 = as.character(route10),
         route11 = as.character(route11))%>%
  pivot_longer(route1:route11, names_to = "route_number", values_to = "route_name")

Route_line = distinct(Route_line, station_name, line, .keep_all = TRUE)

#Distinct stations serve the A train 
Station_A = filter(Route_line, route_name == "A")

#Stations serve the A train and ADA compliant
Station_A_ADA = filter(Route_line, route_name == "A", ada == "A")
```

##### There are 60 distinct stations serve the A train. 0 of the stations that serve the A train are ADA compliant.

# Problem 3

First, clean the data in pols-month

``` r
# separate mon, replace month number with month name
pols_month = 
    read_csv(
        "./data/fivethirtyeight_datasets/pols-month.csv"
        ) %>%
  janitor::clean_names() %>%
  separate(mon, c("year","month","day")) %>%
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day))%>%
  mutate(month = month.abb[month])
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

``` r
# create a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable
pols_month = mutate(pols_month, president = ifelse(prez_gop == "0", "dem","gop"))%>%
  relocate(president, .before = "gov_gop")%>%
  select(-c(3,4,8))
```

Second, clean the data in snp.csv

``` r
# separate date, replace month number with month name, arrange by year and month
snp = 
    read_csv(
        "./data/fivethirtyeight_datasets/snp.csv"
        ) %>%
  janitor::clean_names() %>%
  separate(date, c("month","day","year")) %>%
  mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day))%>%
  arrange(year, month)%>%
  mutate(month = month.abb[month])%>%
  relocate(year, .before = "month")
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

Third, tidy the unemployment data

``` r
unemploy = 
    read_csv(
        "./data/fivethirtyeight_datasets/unemployment.csv"
        )%>%
  pivot_longer(Jan:Dec, names_to = "month", values_to = "unemploy_percent")%>%
  mutate(Year = as.integer(Year))
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

Join the datasets by merging snp into pols, and merging unemployment
into the result.
